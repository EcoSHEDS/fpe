---
title: "FPE Annotations by EPA Interns, May 2023"
author: "Jeff Walker"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
format:
  html:
    self-contained: true
    page-layout: full
    fig-width: 12
    fig-height: 8
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
  cache: true
---

<style>
table.table {
  max-width: 800px;
}
</style>

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(jsonlite)
library(lubridate)
library(janitor)
library(DT)
```

```{r}
# epa intern ids
user_ids <- c(
  "a280a13e-7c8d-4727-9d09-f7eb32f5f415",
  "f2be13dd-29c8-4d06-8063-a64c39b880bf"
)

# flow/images datasets for west brook and avery brook generated by src/export-station.R
flow_images <- bind_rows(
  `29` = read_csv("d:/fpe/westbrook/20230523/West Brook 0_01171100/flow-images.csv", show_col_types = FALSE),
  `12` = read_csv("d:/fpe/westbrook/20230523/Avery Brook_Bridge_01171000/flow-images.csv", show_col_types = FALSE),
  .id = "station_id"
) %>%
  mutate(station_id = as.numeric(station_id)) %>%
  select(-station_name)

# fetch annotations from database
config <- config::get()
con <- DBI::dbConnect(
  RPostgres::Postgres(),
  host = config$db$host,
  port = config$db$port,
  dbname = config$db$database,
  user = config$db$user,
  password = config$db$password
)

annotations_db <- tbl(con, "annotations") %>%
  filter(
    station_id %in% local(flow_images$station_id),
    user_id %in% local(user_ids)
  ) %>%
  left_join(
    select(tbl(con, "stations"), station_id = id, station_name = name),
    by = "station_id"
  ) %>%
  select(annotation_id = id, user_id, station_id, station_name, duration_sec, n, url) %>%
  collect()
DBI::dbDisconnect(con)

annotations <- annotations_db %>%
  rowwise() %>%
  mutate(
    data = list({
      url %>%
        read_json(simplifyVector = TRUE, flatten = TRUE) %>%
        as_tibble() %>%
        mutate(pair_id = row_number())
    })
  ) %>%
  mutate(
    flow_images = list(filter(flow_images, station_id == station_id)),
    user_id = str_sub(user_id, 1, 5),
    data = list({
      data %>%
        mutate(
          left.attributes = map_chr(left.attributes, \(x) str_c(x, collapse = ",")),
          right.attributes = map_chr(right.attributes, \(x) str_c(x, collapse = ","))
        ) %>%
        left_join(
          flow_images %>%
            select(left.imageId = image_id, left.flow_cfs = flow_cfs, left.url = url, left.timestamp = timestamp),
          by = "left.imageId"
        ) %>%
        left_join(
          flow_images %>%
            select(right.imageId = image_id, right.flow_cfs = flow_cfs, right.url = url, right.timestamp = timestamp),
          by = "right.imageId"
        ) %>%
        mutate(
          delta_flow_cfs = abs(left.flow_cfs - right.flow_cfs),
          avg_flow_cfs = (left.flow_cfs + right.flow_cfs) / 2,
          rel_delta_flow_cfs = delta_flow_cfs / avg_flow_cfs,
          true_rank = case_when(
            left.flow_cfs < right.flow_cfs ~ "RIGHT",
            left.flow_cfs > right.flow_cfs ~ "LEFT",
            left.flow_cfs == right.flow_cfs ~ "SAME",
            TRUE ~ NA_character_
          )
        )
    })
  ) %>%
  select(-flow_images) %>%
  unnest(data) %>% 
  rename(user_rank = rank)
```


# Introduction

Two high school interns were recruited by USEPA to annotate FPE photos at two stations (`West Brook 0_01171100` and `Avery Brook_Bridge_01171000`) both of which have observed flow data. Each intern was asked to perform 1,000 annotations at each station.
The goals for this task were to:

1. test the UI/UX of the new image annotation interface,
2. estimate the average speed of a human annotator (how long does it take to annotate each image pair), and
3. estimate the accuracy of human annotators.


# Dataset

For each user and station, the annotation dataset contains the left and right images along, the associated flows for each image, the rank as selected by the user (left, right, same, or unknown) and the true rank based on the observed flows. 

```{r}
annotations %>%
  select(user_id, station_name, left.imageId, left.flow_cfs, right.imageId, right.flow_cfs, user_rank, true_rank) %>% 
  DT::datatable()
```


# Results

Following tables summarize the number of annotations for each category and station. Overall, the two users generated 1,600 annotations for Avery and 2,000 for West Brook. The users were unable to assign an annotation (`UNKNOWN`) for 26% of the pairs at Avery and 10% at West Brook.

```{r}
# tbl-cap: Tallied user annotations
annotations %>%
  tabyl(user_rank, station_name) %>%
  adorn_totals(where = "both") %>% 
  rename(`User Rank` = user_rank) %>% 
  knitr::kable(format.args = list(big.mark = ","), align = "rrrr")
```


```{r}
# tbl-cap: Tallied user annotations as percent of total by station
annotations %>%
  tabyl(user_rank, station_name) %>%
  adorn_percentages(denominator = "col") %>% 
  adorn_totals(where = "row") %>% 
  adorn_pct_formatting(digits = 0) %>% 
  rename(`User Rank` = user_rank) %>% 
  knitr::kable(align = "rrr")
```

## Annotator Accuracy

Annotator accuracy was evaluated by comparing the user-selected rank against the true rank for pairs where the user selected either LEFT or RIGHT. Pairs where the user selected SAME or UNKNOWN were excluded as these cannot be judged for accuracy.

Overall, error rates were less than 10% for both interns and stations. Errors rates were slightly higher at Avery Brook than at West Brook, possibly due to greater shifts in camera position.

```{r}
#| tbl-cap: Annotation error rates
annotations %>%
  filter(user_rank %in% c("LEFT", "RIGHT")) %>%
  mutate(correct = user_rank == true_rank) %>%
  group_by(user_id, station_name) %>% 
  summarize(error_rate = mean(!correct), .groups = "drop") %>% 
  pivot_wider(names_from = "station_name", values_from = "error_rate") %>% 
  adorn_pct_formatting(digits = 1) %>% 
  rename(`User ID` = user_id) %>% 
  knitr::kable(align = "rrr")
```

As expected, most incorrect annotations occurred near the 1:1 line where the two images have similar flows. However, there were a few outliers where the flows were orders of magnitude different (e.g. user f2be1 for West Brook).

At West Brook, most annotation errors occurred when flows were relatively high (> 10 cfs). However, at Avery Brook, one of the two users (a290a) also made a number of errors at low flows (< 2 cfs).

```{r}
#| fig.cap: Annotation accuracy based on left/right image flows
#| fig.height: 8
#| fig.width: 10
annotations %>%
  filter(user_rank %in% c("LEFT", "RIGHT")) %>%
  ggplot(aes(right.flow_cfs, left.flow_cfs)) +
  geom_abline() +
  geom_point(
    data = ~ subset(., user_rank == true_rank),
    aes(color = user_rank == true_rank),
    size = 3, alpha = 0.5
  ) +
  geom_point(
    data = ~ subset(., user_rank != true_rank),
    aes(color = user_rank == true_rank),
    size = 3
  ) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_manual(
    "Annotation\nAccuracy",
    labels = c("TRUE" = "Correct", "FALSE" = "Incorrect"),
    values = c("TRUE" = "gray50", "FALSE" = "orangered")
  ) +
  labs(
    x = "Right Image Flow (cfs)", y = "Left Image Flow (cfs)",
    subtitle = "Annotator accuracy when user selected LEFT or RIGHT (excludes SAME or UNKNOWN)\nRed points shows when annotator chose LEFT or RIGHT, but true comparison was opposite of what they chose"
  ) +
  facet_grid(vars(user_id), vars(station_name), labeller = "label_both") +
  theme_bw()
```

Viewing the same data based on the average flow and relative flow difference between the images in each pair shows that errors are more common at higher flows and also when the difference in flows is relatively small. However, some errors also occurred at low flows and when the relative difference was fairly high (as much as 200%).

```{r}
#| fig.cap: Annotation accuracy based on relative and average flows
annotations %>%
  filter(user_rank %in% c("LEFT", "RIGHT")) %>%
  ggplot(aes(avg_flow_cfs, rel_delta_flow_cfs)) +
  geom_point(
    data = ~ subset(., user_rank == true_rank),
    aes(color = user_rank == true_rank),
    size = 3, alpha = 0.5
  ) +
  geom_point(
    data = ~ subset(., user_rank != true_rank),
    aes(color = user_rank == true_rank),
    size = 3
  ) +
  scale_x_log10() +
  scale_y_continuous(labels = scales::percent) +
  scale_color_manual(
    "Annotation\nAccuracy",
    labels = c("TRUE" = "Correct", "FALSE" = "Incorrect"),
    values = c("TRUE" = "gray50", "FALSE" = "orangered")
  ) +
  labs(
    x = "Average Flow (cfs)\n(Left Flow + Right Flow) / 2", y = "Relative Flow Difference (%)\nabs(Left Flow - Right Flow) / Average Flow",
    subtitle = "EPA Intern Accuracy when annotation is LEFT or RIGHT (excludes SAME, or UNKNOWN)\nRed points shows when annotator chose LEFT or RIGHT, but true comparison was opposite of what they chose"
  ) +
  facet_grid(vars(user_id), vars(station_name), labeller = "label_both") +
  theme_bw()
```

Comparing the cumulative number of annotation errors over the course of each session indicates that errors can sometimes occur relatively evenly over the course of each annotation session, and other times can occur more frequently at the start or end of the session. This figure shows the cumulative fraction of all errors over the course of each session. The two solid lines for user a280a both indicate a rise in error rates (the lines become steeper) both at the beginning and towards the end of the session for each station (beginning around pair # 750). However, the cumulative error curve for the other user (f2be1) and West Brook was relatively straight indicating that errors occurred at about the same rate over the entire session (note: user f2be1 did not complete a 1,000 annotation session for Avery Brook). These results suggest that perhaps some users make more frequent errors in the beginning of a session as they become familiar with the interface and/or station images, or towards the end due to annotation fatigue.

```{r}
#| fig.cap: Cumulative errors over each annotation session
#| fig.width: 8
#| fig.height: 6
annotations %>%
  filter(n == 1000, user_rank %in% c("LEFT", "RIGHT"), user_rank != true_rank) %>%
  arrange(annotation_id, pair_id) %>% 
  group_by(annotation_id) %>% 
  mutate(cumul_error = row_number() / n()) %>% 
  ggplot(aes(pair_id, cumul_error)) +
  geom_line(aes(linetype = user_id, color = station_name)) +
  # scale_color_manual(
  #   "Annotation\nAccuracy",
  #   labels = c("TRUE" = "Correct", "FALSE" = "Incorrect"),
  #   values = c("TRUE" = "gray50", "FALSE" = "orangered")
  # ) +
  scale_color_brewer("Station", palette = "Set1") +
  scale_x_continuous(expand = expansion(), limits = c(0, NA)) +
  scale_y_continuous(expand = expansion(), limits = c(0, NA), labels = scales::percent) +
  labs(
    x = "Image Pair #", y = "Cumulative Fraction of Total Errors",
    linetype = "User ID",
    subtitle = "Cumulative annotation error rate over the course of each session\nOnly includes sessions with 1,000 total annotations"
  ) +
  # facet_wrap(vars(annotation_id, station_name, user_id), labeller = "label_both", scales = "free", ncol = 3) +
  theme_bw()
```

For each user and station, annotation error rates were relatively similar regardless of whether both images within each pair were during the daytime or not EXCEPT for the annotations by user f2be1 at Avery Brook, where the error rate for daytime images (`x=TRUE`) was nearly half of that for one or both images being taken at night. This suggests errors may at times be caused by one or both images taken at night, but not consistently.

```{r}
#| fig.cap: Annotation error rates during day vs night
#| fig.height: 4
annotations %>%
  filter(user_rank %in% c("LEFT", "RIGHT")) %>%
  group_by(
    annotation_id,
    user_id,
    station_name,
    day = hour(right.timestamp) %in% 7:19 & hour(left.timestamp) %in% 7:19
  ) %>% 
  summarise(n_error = sum(user_rank != true_rank) / n(), .groups = "drop") %>% 
  ggplot(aes(day, n_error)) +
  geom_col(aes(fill = user_id), position = "dodge") +
  scale_fill_brewer("User ID", palette = "Set1") +
  scale_y_continuous(expand = expansion(c(0, 0.05)), limits = c(0, NA), labels = scales::percent) +
  labs(x = "Both Left/Right Images During Daytime (7AM-7PM)", y = "Annotation Error Rate") +
  facet_wrap(vars(station_name), labeller = "label_both") +
  theme_bw()
```

## Annotator Margin

The annotator "margin" refers to how different flows need to be between two images for the annotator to tell the difference. When the flow difference is below the "margin" then the annotator will indicate that the two images have "about the same" flow. 

Plotting the relative flow difference against the average flow of each pair shows that there was a wide range of relative flow differences for which users were unable to distinguish a difference between the two images. However, as expected, these relative flow differences when users selected `SAME` are skewed to the right meaning smaller flow differences were more common. But in some cases, users selected `SAME` when the relative flow difference between two images was more than 150%. 

Interestingly, the distribution of `SAME` annotations is consistent across both sites and users, as well as over the range of flows. This suggests that the ability for annotators to distinguish flows may be relatively consistent over a wide range of flows. 

```{r}
#| fig.cap: Margin distribution based on average flows and relative flow difference of image pairs
annotations %>%
  filter(user_rank != "UNKNOWN") %>%
  ggplot(aes(avg_flow_cfs, rel_delta_flow_cfs, color = user_rank)) +
  geom_point(
    data = ~ subset(., user_rank != "SAME"),
    aes(color = user_rank),
    size = 3, alpha = 0.5
  ) +
  geom_point(
    data = ~ subset(., user_rank == "SAME"),
    aes(color = user_rank),
    size = 3
  ) +
  scale_x_log10() +
  scale_y_continuous(labels = scales::percent, breaks = scales::pretty_breaks(n = 8)) +
  scale_color_manual("Annotation\nRank", values = c("LEFT" = "gray50", "RIGHT" = "gray50", "SAME" = "deepskyblue")) +
  labs(
    x = "Average Flow (cfs)\n(Left Flow + Right Flow) / 2", y = "Relative Flow Difference (%)\nabs(Left Flow - Right Flow) / Average Flow"
  ) +
  facet_grid(vars(user_id), vars(station_name), labeller = "label_both") +
  theme_bw()
```

```{r}
mean_rel_delta_flow_cfs <- mean(filter(annotations, user_rank == "SAME")$rel_delta_flow_cfs)
```


The cumulative distribution of the relative flow difference when annotators selected SAME indicates:

1. Overall mean of the relative flow differences across both users and sites was `r scales::percent(mean_rel_delta_flow_cfs)`, which is a bit higher than we expected.
2. The median of the flow differences was between 35-45% for both annotators and both sites, meaning among all the image pairs when a user selected SAME, approximately half had a relative flow difference of 40% or less. 
3. The distributions were similar between the two users at West Brook (blue lines are similar), but somewhat different for Avery Brook (red lines diverge).

```{r}
#| fig.cap: Cumulative frequency distribution of relative flow difference when users selected SAME
#| fig.width: 8
#| fig.height: 6
annotations %>%
  filter(user_rank == "SAME") %>%
  ggplot(aes(rel_delta_flow_cfs, linetype = user_id, color = station_name)) +
  stat_ecdf() +
  scale_color_brewer("Station", palette = "Set1") +
  scale_x_continuous(labels = scales::percent, breaks = scales::pretty_breaks(n = 8), limits = c(0, NA), expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = scales::pretty_breaks(n = 8), expand = expansion()) +
  labs(
    x = "Relative Flow Difference (%)\nabs(Left Flow - Right Flow) / Average Flow",
    y = "Cumulative Frequency",
    linetype = "User ID"
  ) +
  theme_bw()
```

```{r}
annotations %>% 
  write_csv("annotations-epa-interns-may-2023.csv")
```

